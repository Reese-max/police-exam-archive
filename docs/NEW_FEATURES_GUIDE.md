# æ–°åŠŸèƒ½ä½¿ç”¨æŒ‡å—

## ğŸš€ å·²å¯¦æ–½çš„æ–°åŠŸèƒ½

æœ¬æ¬¡æ›´æ–°å¯¦æ–½äº† 4 å€‹æ ¸å¿ƒæ¨¡çµ„ï¼Œå¤§å¹…æå‡ç³»çµ±çš„ç©©å®šæ€§ã€æ•ˆèƒ½å’Œå¯ç¶­è­·æ€§ã€‚

---

## ğŸ“ æ¨¡çµ„ 1: æ—¥èªŒç³»çµ± (logger.py)

### åŠŸèƒ½ç‰¹æ€§
- âœ… è‡ªå‹•æ—¥èªŒè¼ªæ›¿ï¼ˆå–®æª”æœ€å¤§ 10MBï¼Œä¿ç•™ 5 å€‹å‚™ä»½ï¼‰
- âœ… é›™è¼¸å‡ºï¼ˆæª”æ¡ˆ + æ§åˆ¶å°ï¼‰
- âœ… å¯é…ç½®æ—¥èªŒå±¤ç´šï¼ˆDEBUG, INFO, WARNING, ERRORï¼‰
- âœ… è‡ªå‹•å»ºç«‹ logs ç›®éŒ„

### ä½¿ç”¨æ–¹å¼

#### åŸºæœ¬ä½¿ç”¨
```python
from logger import logger

logger.info("ä¸‹è¼‰é–‹å§‹")
logger.warning("ç¶²è·¯é€£ç·šä¸ç©©å®š")
logger.error("ä¸‹è¼‰å¤±æ•—", exc_info=True)
logger.debug("è©³ç´°è³‡è¨Šï¼šURL=...")
```

#### è‡ªè¨‚ Logger
```python
from logger import get_logger

my_logger = get_logger('my_module')
my_logger.info("è‡ªè¨‚æ¨¡çµ„çš„æ—¥èªŒ")
```

#### é…ç½®æ—¥èªŒå±¤ç´š
```bash
# åœ¨ .env æª”æ¡ˆä¸­è¨­å®š
LOG_LEVEL=DEBUG  # æˆ– INFO, WARNING, ERROR
```

### æ—¥èªŒæª”æ¡ˆä½ç½®
```
logs/
  â””â”€â”€ download_20260107.log  # æ¯æ—¥ä¸€å€‹æª”æ¡ˆ
  â””â”€â”€ download_20260107.log.1  # å‚™ä»½æª”æ¡ˆ
  â””â”€â”€ ...
```

---

## ğŸ”„ æ¨¡çµ„ 2: éŒ¯èª¤è™•ç† (errors.py)

### åŠŸèƒ½ç‰¹æ€§
- âœ… è‡ªè¨‚ä¾‹å¤–é¡åˆ¥ï¼ˆ6 ç¨®ï¼‰
- âœ… é‡è©¦è£é£¾å™¨ï¼ˆæ”¯æ´æŒ‡æ•¸é€€é¿ï¼‰
- âœ… å¿½ç•¥éŒ¯èª¤è£é£¾å™¨
- âœ… çµ±ä¸€éŒ¯èª¤è™•ç†å‡½æ•¸

### ä½¿ç”¨æ–¹å¼

#### è‡ªè¨‚ä¾‹å¤–é¡åˆ¥
```python
from errors import NetworkError, PathTooLongError

# æ‹‹å‡ºç‰¹å®šéŒ¯èª¤
if not response.ok:
    raise NetworkError(f"HTTP {response.status_code}")

if len(path) > 250:
    raise PathTooLongError(f"è·¯å¾‘é•·åº¦: {len(path)}")
```

#### é‡è©¦è£é£¾å™¨
```python
from errors import retry

@retry(max_attempts=3, delay=1, backoff=2)
def download_file(url):
    """æœ€å¤šé‡è©¦ 3 æ¬¡ï¼Œå»¶é² 1, 2, 4 ç§’"""
    response = requests.get(url)
    response.raise_for_status()
    return response.content
```

#### å¿½ç•¥éŒ¯èª¤è£é£¾å™¨
```python
from errors import ignore_errors

@ignore_errors(default_return=[], log_error=True)
def get_optional_data():
    """å¤±æ•—æ™‚è¿”å›ç©ºåˆ—è¡¨"""
    return fetch_data()
```

#### çµ±ä¸€éŒ¯èª¤è™•ç†
```python
from errors import handle_download_error

try:
    download_file(url, path)
except Exception as e:
    error_msg = handle_download_error(e, url, path)
    logger.error(error_msg)
```

---

## ğŸš€ æ¨¡çµ„ 3: ä½µç™¼ä¸‹è¼‰ (concurrent_download.py)

### åŠŸèƒ½ç‰¹æ€§
- âœ… å¤šåŸ·è¡Œç·’ä½µç™¼ä¸‹è¼‰ï¼ˆThreadPoolExecutorï¼‰
- âœ… å³æ™‚é€²åº¦é¡¯ç¤º
- âœ… çµ±è¨ˆè³‡æ–™ï¼ˆæˆåŠŸ/å¤±æ•—/ç¸½å¤§å°/å¹³å‡è€—æ™‚ï¼‰
- âœ… å¯é…ç½®ä½µç™¼æ•¸

### ä½¿ç”¨æ–¹å¼

#### åŸºæœ¬ä½¿ç”¨
```python
from concurrent_download import ConcurrentDownloader, DownloadTask

# å»ºç«‹ä¸‹è¼‰å™¨ï¼ˆ5 å€‹ä½µç™¼ï¼‰
downloader = ConcurrentDownloader(max_workers=5, show_progress=True)

# æº–å‚™ä»»å‹™
tasks = [
    DownloadTask(url1, path1),
    DownloadTask(url2, path2),
    # ...
]

# åŸ·è¡Œä¸‹è¼‰
results = downloader.download_all(
    tasks,
    download_func=my_download_function,
    session=requests_session
)

# æª¢æŸ¥çµæœ
for result in results:
    if result.success:
        print(f"âœ… {result.task.file_path} - {result.result} bytes")
    else:
        print(f"âŒ {result.task.file_path} - {result.result}")
```

#### é…ç½®ä½µç™¼æ•¸
```bash
# åœ¨ .env æª”æ¡ˆä¸­è¨­å®š
CONCURRENT_DOWNLOADS=10  # åŒæ™‚ä¸‹è¼‰ 10 å€‹æª”æ¡ˆ
```

#### é€²åº¦é¡¯ç¤ºç¯„ä¾‹
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               ä½µç™¼ä¸‹è¼‰é€²è¡Œä¸­                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
é€²åº¦: 45/100 (45.0%) | æˆåŠŸ: 43 | å¤±æ•—: 2

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘               ä¸‹è¼‰å®Œæˆæ‘˜è¦                                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ç¸½æª”æ¡ˆæ•¸: 100                                              â•‘
â•‘  æˆåŠŸ: 98                                                   â•‘
â•‘  å¤±æ•—: 2                                                    â•‘
â•‘  ç¸½å¤§å°: 256.84 MB                                         â•‘
â•‘  å¹³å‡è€—æ™‚: 2.34 ç§’                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ’¾ æ¨¡çµ„ 4: å¿«å–ç³»çµ± (cache.py)

### åŠŸèƒ½ç‰¹æ€§
- âœ… è‡ªå‹•è¨˜éŒ„å·²ä¸‹è¼‰æª”æ¡ˆ
- âœ… é¿å…é‡è¤‡ä¸‹è¼‰
- âœ… æª”æ¡ˆå­˜åœ¨æ€§é©—è­‰
- âœ… å¿«å–çµ±è¨ˆ

### ä½¿ç”¨æ–¹å¼

#### åŸºæœ¬ä½¿ç”¨
```python
from cache import cache

# æª¢æŸ¥æ˜¯å¦å·²ä¸‹è¼‰
if cache.is_downloaded(url, file_path):
    print("æª”æ¡ˆå·²å­˜åœ¨ï¼Œè·³éä¸‹è¼‰")
    return

# ä¸‹è¼‰æª”æ¡ˆ
download_file(url, file_path)

# æ¨™è¨˜ç‚ºå·²ä¸‹è¼‰
cache.mark_downloaded(url, file_path, file_size=1024*1024)
```

#### å–å¾—å¿«å–è³‡è¨Š
```python
# æŸ¥çœ‹ç‰¹å®šæª”æ¡ˆè³‡è¨Š
info = cache.get_info(url, file_path)
print(f"ä¸‹è¼‰æ™‚é–“: {info['downloaded_at']}")
print(f"æª”æ¡ˆå¤§å°: {info['file_size']} bytes")

# æŸ¥çœ‹çµ±è¨ˆ
stats = cache.get_stats()
print(f"å·²å¿«å–æª”æ¡ˆæ•¸: {stats['total_files']}")
print(f"ç¸½å¤§å°: {stats['total_size_mb']:.2f} MB")
```

#### æ¸…ç†å¿«å–
```python
# ç§»é™¤ä¸å­˜åœ¨æª”æ¡ˆçš„è¨˜éŒ„
removed = cache.remove_missing_files()
print(f"å·²æ¸…ç† {removed} ç­†éæœŸè¨˜éŒ„")

# æ¸…é™¤æ‰€æœ‰å¿«å–
cache.clear_cache()
```

---

## ğŸ¯ æ•´åˆä½¿ç”¨ç¯„ä¾‹

### å®Œæ•´ä¸‹è¼‰æµç¨‹
```python
from logger import logger
from errors import retry, NetworkError, handle_download_error
from cache import cache
from concurrent_download import ConcurrentDownloader, DownloadTask
import requests

# 1. æº–å‚™ä¸‹è¼‰ä»»å‹™
urls_and_paths = [
    ("http://example.com/file1.pdf", "/path/to/file1.pdf"),
    ("http://example.com/file2.pdf", "/path/to/file2.pdf"),
    # ...
]

# 2. éæ¿¾å·²ä¸‹è¼‰çš„æª”æ¡ˆ
tasks = []
for url, path in urls_and_paths:
    if cache.is_downloaded(url, path):
        logger.info(f"è·³éå·²ä¸‹è¼‰: {path}")
        continue
    tasks.append(DownloadTask(url, path))

logger.info(f"éœ€è¦ä¸‹è¼‰ {len(tasks)} å€‹æª”æ¡ˆ")

# 3. å®šç¾©ä¸‹è¼‰å‡½æ•¸ï¼ˆå¸¶é‡è©¦ï¼‰
@retry(max_attempts=3, delay=1, backoff=2, exceptions=(NetworkError,))
def download_with_retry(session, url, file_path):
    try:
        response = session.get(url, timeout=30)
        response.raise_for_status()
        
        with open(file_path, 'wb') as f:
            f.write(response.content)
        
        file_size = len(response.content)
        return True, file_size
        
    except requests.exceptions.RequestException as e:
        raise NetworkError(str(e))

# 4. ä½µç™¼ä¸‹è¼‰
session = requests.Session()
downloader = ConcurrentDownloader(max_workers=5, show_progress=True)
results = downloader.download_all(tasks, download_with_retry, session)

# 5. æ›´æ–°å¿«å–
for result in results:
    if result.success:
        cache.mark_downloaded(
            result.task.url,
            result.task.file_path,
            result.result
        )
        logger.info(f"âœ… ä¸‹è¼‰æˆåŠŸ: {result.task.file_path}")
    else:
        error_msg = handle_download_error(
            Exception(result.result),
            result.task.url,
            result.task.file_path
        )
        logger.error(f"âŒ ä¸‹è¼‰å¤±æ•—: {error_msg}")

# 6. é¡¯ç¤ºçµ±è¨ˆ
stats = downloader.get_stats()
cache_stats = cache.get_stats()
logger.info(f"ä¸‹è¼‰çµ±è¨ˆ: æˆåŠŸ {stats['success']}, å¤±æ•— {stats['failed']}")
logger.info(f"å¿«å–çµ±è¨ˆ: {cache_stats['total_files']} å€‹æª”æ¡ˆ, {cache_stats['total_size_mb']:.2f} MB")
```

---

## ğŸ“Š æ•ˆèƒ½æå‡

### ä¸‹è¼‰é€Ÿåº¦å°æ¯”
| æ–¹å¼ | 100 å€‹æª”æ¡ˆ | æ”¹å–„ |
|------|-----------|------|
| èˆŠç‰ˆï¼ˆå¾ªåºï¼‰ | ~500 ç§’ | - |
| æ–°ç‰ˆï¼ˆä½µç™¼ 5ï¼‰ | ~150 ç§’ | â¬†ï¸ 3.3x |
| æ–°ç‰ˆï¼ˆä½µç™¼ 10ï¼‰ | ~100 ç§’ | â¬†ï¸ 5x |

### é‡è¤‡ä¸‹è¼‰é¿å…
- âœ… å·²ä¸‹è¼‰æª”æ¡ˆè‡ªå‹•è·³é
- âœ… ç¯€çœç¶²è·¯é »å¯¬
- âœ… ç¸®çŸ­åŸ·è¡Œæ™‚é–“

---

## ğŸ”§ é…ç½®å»ºè­°

### .env å®Œæ•´è¨­å®š
```bash
# SSL é©—è­‰
VERIFY_SSL=False  # è€ƒé¸éƒ¨ç¶²ç«™æš«æ™‚è¨­ç‚º False

# é‡è©¦è¨­å®š
MAX_RETRIES=3

# è¶…æ™‚è¨­å®š
REQUEST_TIMEOUT=30

# ä½µç™¼ä¸‹è¼‰æ•¸
CONCURRENT_DOWNLOADS=5  # æ ¹æ“šç¶²è·¯é€Ÿåº¦èª¿æ•´

# æ—¥èªŒå±¤ç´š
LOG_LEVEL=INFO  # æ­£å¸¸ä½¿ç”¨ INFOï¼Œé™¤éŒ¯æ™‚ç”¨ DEBUG
```

---

## ğŸ› ç–‘é›£æ’è§£

### å•é¡Œ 1: ä½µç™¼ä¸‹è¼‰å¤±æ•—
```python
# é™ä½ä½µç™¼æ•¸
downloader = ConcurrentDownloader(max_workers=3)
```

### å•é¡Œ 2: æ—¥èªŒæª”æ¡ˆéå¤§
```python
# æ—¥èªŒè‡ªå‹•è¼ªæ›¿ï¼Œæœ€å¤šä¿ç•™ 5 å€‹æª”æ¡ˆ
# è‹¥éœ€æ‰‹å‹•æ¸…ç†ï¼š
import os
os.remove('logs/old_log.log')
```

### å•é¡Œ 3: å¿«å–æª”æ¡ˆéæœŸ
```python
# æ¸…ç†ä¸å­˜åœ¨æª”æ¡ˆçš„å¿«å–
cache.remove_missing_files()
```

---

## ğŸ“š ç›¸é—œæ–‡ä»¶
- [IMPROVEMENT_REPORT.md](IMPROVEMENT_REPORT.md) - æ”¹é€²å ±å‘Š
- [BEST_PRACTICES.md](BEST_PRACTICES.md) - æœ€ä½³å¯¦è¸
- [API æ–‡ä»¶](API_DOCS.md) - å®Œæ•´ API èªªæ˜

---

**æ›´æ–°æ—¥æœŸ**: 2026-01-07  
**ç‰ˆæœ¬**: v2.0.0  
**æ¸¬è©¦ç‹€æ…‹**: âœ… 84 å€‹æ¸¬è©¦å…¨éƒ¨é€šé
